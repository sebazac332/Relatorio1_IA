<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Relatorio 1 Inteligencia Artificial</title>
</head>
<body>
    <div class="navbar">
        <a href="index.html">O que é inteligência artificial?</a>
        <a href="history.html">Histórico.</a>
        <a href="art_pros_cons.html">Estado da Arte, Benefícios e Riscos.</a>
        <a href="agentes_ambientes.html">Agentes e ambientes.</a>
        <a href="index.html">Racionalidade.</a>
        <a href="index.html">Especificando o ambiente de tarefas.</a>
        <a href="index.html">Arquitetura do agente.</a>
        <a href="index.html">Representação de estados</a>
        <a href="index.html">Discussões</a>
        <a href="index.html">Projetos e problemas</a>
    </div>

    <h1>Histórico</h1>

    <h2>Mitologia e Ficção</h2>
    <p>
        A ideia de ter seres artificiais que fazem tarefas para nós não é recente, algumas das representações mais antigas de 
        inteligência artificial podem ser encontradas em mitos, legendas e folclore. Alguns exemplos disso seriam Talos, um 
        gigante de bronze da mitologia grega que foi construído por Hefesto para defender Creta, outro exemplo seria os golems 
        da mitologia judaica, que foram criados através de rituais místicos para proteger seu criador, também aqui poderia considerar 
        Pandora também da mitologia grega e também criada por Hefesto que, embora fosse muito semelhante a uma pessoa também foi 
        criada com um propósito específico, abrir a caixa que continha todos os males, como uma forma de punição divina para a 
        humanidade.

        Através destas peças de mitologia pode apreciar bastante das características modernas das inteligências artificiais como a 
        divisão entre IA focadas no razocinio e aquelas que tentam imitar o comportamento humano e como todas as IA foram criadas 
        para lidar com um propósito específico.

        Nos campos da ficção existe uma grande prevalência de histórias onde as IA se levantam contra seus criadores e tentam 
        destruí-los, alguns exemplos são Skynet de Terminator, HAL 9000 de 2001: Uma odisseia espacial e Ultron de Marvel; aqui 
        está plasmada uma das maiores preocupações com o desenvolvimento da IA, a qual é a possível perda de controle e no caso 
        específico de Ultron encontra-se um cenário extremo de uma falha no modelo padrão; o principal objetivo definido para Ultron 
        é proteger a terra de qualquer ameaça alienígena e doméstica, no entanto, durante o processo de definição de objetivos nunca 
        foi colocado limites às medidas que Ultron poderia tomar em relação à raça humana; Isso levou Ultron a começar um genocídio 
        contra todos os humanos, pois ele considerou que eles eram a maior ameaça para a terra.
    </p>
</body>

<h2>O início da inteligência artificial (1943-1956)</h2>
<p>
    Foi durante este período que o primeiro modelo de redes neuronais foi criado, Warren McCulloch e Walter Pitts propuseram um modelo 
    de neurônios artificiais em que cada um é caracterizado por estar ligado ou desligado, Isso acontece dependendo do estado em que 
    os neurônios adjacentes estão. Uma adição importante a este modelo foi de Donald Hebb que demonstrou uma regra para a modificação 
    da força da conexão entre neurônios chamado Hebbian learning, o qual é influencial até agora. A primeira rede neural real foi 
    construída por Marvin Minsky e Dean Edmonds em 1950.

    Alan Turing também fez contribuições muito importantes introduzindo o teste de Turing, machine learning, algoritmos genéticos e 
    aprendizado por reforço; também levantei a noção de que para programar inteligência artificial de nível humano era mais fácil 
    criar algoritmos de aprendizagem e ensinar a máquina do que programar a inteligência manualmente.

    Foi durante este período que também realizou o workshop de Dartmouth; um estudo sobre inteligência artificial que duraria 2 meses 
    e tinha 10 participantes, onde todos eram pesquisadores, lamentavelmente este workshop não levou a nenhum avanço, o trabalho mais 
    completo foi apresentado por Newell e Simon, que consistiu em um programa de computador capaz de pensar não-numericamente.
</p>

<h2>Entusiasmo inicial, grandes expectativas (1952-1969)</h2>
<p>
    Foi durante este período que os pesquisadores de IA quebraram muitos dos conceitos que tinham sobre os limites do que uma IA 
    poderia realizar, isto foi feito com um foco em atividades que eram consideradas indicativas da inteligência humana como são 
    jogos, quebra-cabeças, matemática e outras.

    Foi durante este período que Newell e Simon criaram o GPS (General problem solver) resolvedor de problemas geral, capaz de 
    resolver alguns tipos de quebra-cabeças tinha a peculiaridade de considerar ações possíveis e objetivos de uma forma semelhante 
    como uma pessoa faria; foi um dos primeiros programas a usar a abordagem de pensamento semelhante ao humano. Isso levou à criação 
    de uma hipótese chamada sistema físico de símbolos (physical symbol system) que postulava que qualquer sistema que mostrasse 
    inteligência deveria operar manipulando os símbolos que representam dados; Um exemplo disso é encontrado na linguagem onde os 
    símbolos são letras, as palavras são expressões e os processos são as ações de falar, expressar e ler.

    Outra criação bastante importante foi feita por Nathaniel Rochester e sua equipe que era o testador de teoremas geométricos 
    (Geometric Theorem Prover) que foi capaz de provar teoremas bastante complicados mesmo para pessoas treinadas em matemática.

    Arthur Samuel foi capaz de criar um programa com a capacidade de jogar damas em um nível amador forte, foi um dos primeiros usos 
    dos métodos que depois se conhecerão como aprendizagem por reforço; foi aqui quando a noção de que os computadores só podem 
    fazer o que lhes é ordenado foi desmentida.

    Dois dos mais importantes avanços deste período foram feitos por John McCarthy, um foi a criação da linguagem de alto nível Lisp 
    que seria a linguagem dominante para IA pelos próximos 30 anos o outro seria a ideia de um programa chamado o Advice Taker, que 
    teria conhecimento geral do mundo e seria capaz de usá-lo para gerar planos e também seria capaz de integrar novos conhecimentos 
    a esse plano permitindo-lhe alcançar competência em áreas desconhecidas sem a necessidade de ser reprogramada; este último 
    influenciou o curso que tomaria o desenvolvimento de IAs pois introduziu a noção de representações objetivas do mundo e como 
    este funciona que depois poderão ser manipuladas para gerar previsões.
</p>

<h2>Uma dose de realidade (1966-1973)</h2>
<p>
    Foi um período caracterizado pela estagnação no desenvolvimento de inteligências artificiais, não se conseguiu que estas pudessem 
    resolver problemas mais complexos do que os já resolvidos, o progresso nulo no desenvolvimento de IAs levou muitos lugares a optar 
    por deixar as pesquisas sobre esses temas.

    Este fracasso foi principalmente por duas razões, a primeira é que as IAs até então se concentravam em como os humanos resolviam 
    tais problemas ao invés de tentar entender o problema em si, definir que uma solução para esse problema poderia ser considerada e 
    que métodos deveriam ser usados para obter tais soluções de forma consistente.

    A segunda razão foi a abordagem que se tomou para resolver problemas, em um início a forma de encontrar soluções era a de tentar 
    diferentes combinações até que uma deu com a resposta, Isso funcionou porque os problemas iniciais contavam com um número muito 
    limitado de objetos e, portanto, tinham poucas ações possíveis e sequências de solução muito curtas.

    Havia a crença errônea de que para resolver problemas maiores, só precisava de hardware mais rápido e memórias maiores.

    Outro problema que contribuiu para esse fracasso foi a limitada capacidade de representação das redes neuronais da época; estas 
    eram capazes de aprender tudo o que podiam representar mas não eram capazes de representar muitas coisas.
</p>

<h2>Sistemas Especialistas (1969-1986)</h2>
<p>
    Durante este período há uma mudança nos métodos de resolução de problemas, a forma utilizada anteriormente baseia-se no uso de 
    conhecimentos gerais para tentar encontrar soluções completas, Este método é conhecido como métodos fracos porque eles são bons 
    para uso geral, mas não são capazes de resolver problemas grandes ou complexos; a alternativa adotada é usar informações mais 
    específicas em relação a um campo ou tema o que permite melhor raciocínio e uma maior facilidade para lidar com situações que 
    ocorrem dentro de um campo ou disciplina específica, estes são conhecidos como métodos fortes.

    Um programa criado com estes novos métodos fortes foi DENDRAL que encontrou a estrutura molecular usando informação dada por um 
    espectrômetro de massas, para isso era necessária a fórmula da moleca e o espectro de massa quando bombardeada por um raio de 
    elétrons; o programa gera todas as possíveis estruturas da fórmula, prediz o espectro de massa de cada uma e compara com o espectro 
    atual para identificar a estrutura molecular correta. Foi o primeiro sistema de conhecimento intensivo.

    Outro sistema foi MYCIN que foi usado para diagnosticar infecções de sangue, seu desempenho foi melhor do que os médicos júnior 
    e estava no nível de doutores experientes, as regras deste sistema foram adquiridos através de entrevistas a profissionais, isto 
    combinado com cálculos de incerteza foram os principais responsáveis pelo seu alto desempenho.

    O primeiro sistema comercial deste tipo foi R1, que serviu para ajudar a configurar pedidos para novos sistemas de computadores 
    selecionando os componentes em base às exigências do cliente, foi um grande sucesso economizando aproximadamente 40 milhões de 
    dólares por ano.

    O sucesso desses sistemas aumentou o valor da indústria de inteligência artificial em bilhões de dólares e levou à criação de 
    muitas empresas baseadas na construção de sistemas especializados; empresas que eventualmente fecharão porque criar e manter 
    sistemas de especialistas para domínios complexos era difícil, este período será chamado o inverno da IA.
</p>

<h2>O retorno das redes neurais</h2>
<p>
    Neste período é criado o chamado modelo conexionista, resultado da reinvenção do algoritmo de aprendizagem por propagação inversa, 
    este modelo é capaz de formar conceitos internos de uma forma mais fluida e imprecisa que se adapta muito melhor à desordem do 
    mundo real, também são capazes de aprender através de exemplos comparando o valor de saída predito com o valor real de um problema 
    modificando os parâmetros para reduzir a diferença.
</p>

<h2>Raciocínio probabilístico e aprendizado de máquina (1987-presente)</h2>
<p>
    A fragilidade dos sistemas especializados levou outra nova abordagem no desenvolvimento de IAs, incorporou-se a probabilidade 
    e em vez do conhecimento adicionado manualmente dos sistemas especializados utiliza-se machine learning para que aprendam por si 
    mesmos; muito mais peso é colocado em resultados experimentais.

    Durante este período, tornou-se mais comum construir sobre teoremas existentes do que a criação de novos teoremas, deixou de lado a 
    intuição e favoreceu-se teoremas sólidos e experimentação rigorosa quando se fazem promessas sobre as capacidades de IA; IAs agora 
    são testados com situações do mundo real ao invés de usar dados gerados pelos mesmos usuários.

    Tornou-se a norma usar conjuntos de problemas compartilhados para medir o desempenho e evolução, alguns destes foram o 
    International Planning Competition para algoritmos de planejamento, LibriSpeech para reconhecimento auditivo da linguagem, 
    MNIST para reconhecimento de digitação escrita, ImageNet e COCO para reconhecimento de objetos, entre outros.

    Foi durante este período que o isolacionismo em que se encontrava o campo da investigação de IAs em relação ao resto das ciências 
    da computação começa a abandonar-se, isto possibilita o uso dos resultados de anos de pesquisa e campos como teoria da informação,
    modelação estocástica, otimização e controle clássico, análise estática entre outros.

    Um dos principais resultados disso foi a criação de redes bayesianas por parte de Judea Pearl, que era capaz de representar 
    conhecimento incerto e também possuía algoritmos práticos para raciocínio probabilístico.

    O processo de integração do campo da IA com os outros campos das ciências da computação trouxe benefícios em termos de aplicações 
    práticas e na melhor compreensão teórica dos problemas centrais da IA.
</p>

<h2>Big Data (2001-presente)</h2>
<p>
    Devido aos avanços na potência computacional e a criação da rede informática mundial (world wide web) se facilitou a criação de 
    grandes conjuntos de dados, este fenômeno passou a ser conhecido como big data.

    Esses conjuntos de dados são compostos por trilhões de palavras, imagens, milhões de horas de vídeo entre outros, estes 
    apresentaram uma oportunidade para que se desenvolvam IAs especialmente projetadas para lidar com quantidades imensuráveis de dados.

    Mesmo sem esses dados terem qualquer tipo de contexto adicionado uma IA com um algoritmo de aprendizagem apropriado será capaz de 
    determinar onde e como eles são usados e também o significado destes com um alto grau de precisão, aproximadamente 96%; para isso 
    é necessária a grande quantidade de dados que oferecem os conjuntos de dados de big data, tal processo não seria possível com 
    quantidades menores de dados.

    A melhoria do desempenho de IAs deste tipo depende muito mais da quantidade de dados que recebem do que dos ajustes que podem ser 
    feitos ao algoritmo.
</p>

<h2>Deep learning (2011-presente)</h2>
<p>
    Deep learning refere-se ao aprendizado de máquina usando múltiplas camadas de elementos de computação simples e ajustáveis, Este 
    novo modelo de IA provou ser superior aos anteriores desenvolvidos como pôde ser visto na competição do ImageNet de 2012 onde o 
    sistema de deep learning criado pelo grupo de Geoffrey Hinton mostrou um desempenho superior em relação aos demais.

    Estes sistemas apresentam desempenho muito melhor do humano em tarefas visuais e estão ligeiramente atrasados em outros aspectos, 
    o sucesso destes sistemas rejuvenesceu os interesses em IA por parte de estuciantes, empresas, governos, o público geral e muitas 
    outras organizações.

    Deep learning requer hardware muito poderoso para funcionar, pois podem consumir entre 10 14 a 10 17 operações por segundo na forma 
    de operações matriciais e vetoriais altamente paralelizadas; Além disso, ele também precisa de uma grande quantidade de dados para 
    o treinamento.
</p>
</html>